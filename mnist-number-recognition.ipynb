{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18db429c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-05T21:46:25.365661Z",
     "iopub.status.busy": "2023-01-05T21:46:25.365180Z",
     "iopub.status.idle": "2023-01-05T21:46:25.379022Z",
     "shell.execute_reply": "2023-01-05T21:46:25.377814Z"
    },
    "papermill": {
     "duration": 0.023747,
     "end_time": "2023-01-05T21:46:25.381684",
     "exception": false,
     "start_time": "2023-01-05T21:46:25.357937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/digit-recognizer/sample_submission.csv\n",
      "/kaggle/input/digit-recognizer/train.csv\n",
      "/kaggle/input/digit-recognizer/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a1390fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-05T21:46:25.392287Z",
     "iopub.status.busy": "2023-01-05T21:46:25.391678Z",
     "iopub.status.idle": "2023-01-05T21:46:31.417222Z",
     "shell.execute_reply": "2023-01-05T21:46:31.415923Z"
    },
    "papermill": {
     "duration": 6.034176,
     "end_time": "2023-01-05T21:46:31.420280",
     "exception": false,
     "start_time": "2023-01-05T21:46:25.386104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a684a547",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-05T21:46:31.430489Z",
     "iopub.status.busy": "2023-01-05T21:46:31.429898Z",
     "iopub.status.idle": "2023-01-05T21:47:03.535178Z",
     "shell.execute_reply": "2023-01-05T21:47:03.533840Z"
    },
    "papermill": {
     "duration": 32.113631,
     "end_time": "2023-01-05T21:47:03.538133",
     "exception": false,
     "start_time": "2023-01-05T21:46:31.424502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_data = np.genfromtxt('/kaggle/input/digit-recognizer/train.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f06272d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-05T21:47:03.549316Z",
     "iopub.status.busy": "2023-01-05T21:47:03.548842Z",
     "iopub.status.idle": "2023-01-05T21:47:03.554288Z",
     "shell.execute_reply": "2023-01-05T21:47:03.553126Z"
    },
    "papermill": {
     "duration": 0.014034,
     "end_time": "2023-01-05T21:47:03.556822",
     "exception": false,
     "start_time": "2023-01-05T21:47:03.542788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels = train_data[1:,0]\n",
    "train_data = train_data[1:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dc9b0f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-05T21:47:03.567569Z",
     "iopub.status.busy": "2023-01-05T21:47:03.567133Z",
     "iopub.status.idle": "2023-01-05T21:47:03.572879Z",
     "shell.execute_reply": "2023-01-05T21:47:03.571780Z"
    },
    "papermill": {
     "duration": 0.01386,
     "end_time": "2023-01-05T21:47:03.575129",
     "exception": false,
     "start_time": "2023-01-05T21:47:03.561269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc437a0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-05T21:47:03.585552Z",
     "iopub.status.busy": "2023-01-05T21:47:03.585102Z",
     "iopub.status.idle": "2023-01-05T21:47:03.591653Z",
     "shell.execute_reply": "2023-01-05T21:47:03.590419Z"
    },
    "papermill": {
     "duration": 0.014847,
     "end_time": "2023-01-05T21:47:03.594309",
     "exception": false,
     "start_time": "2023-01-05T21:47:03.579462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.reshape(-1,28,28,1)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97303df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-05T21:47:03.605046Z",
     "iopub.status.busy": "2023-01-05T21:47:03.604636Z",
     "iopub.status.idle": "2023-01-05T21:47:04.764783Z",
     "shell.execute_reply": "2023-01-05T21:47:04.763447Z"
    },
    "papermill": {
     "duration": 1.169113,
     "end_time": "2023-01-05T21:47:04.767922",
     "exception": false,
     "start_time": "2023-01-05T21:47:03.598809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_array = keras.utils.to_categorical(train_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f294cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-05T21:47:04.778827Z",
     "iopub.status.busy": "2023-01-05T21:47:04.778384Z",
     "iopub.status.idle": "2023-01-05T21:47:04.784940Z",
     "shell.execute_reply": "2023-01-05T21:47:04.783715Z"
    },
    "papermill": {
     "duration": 0.015081,
     "end_time": "2023-01-05T21:47:04.787478",
     "exception": false,
     "start_time": "2023-01-05T21:47:04.772397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 10)\n",
      "[1. 0. 1. ... 7. 6. 9.]\n"
     ]
    }
   ],
   "source": [
    "print(training_array.shape)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe25cff",
   "metadata": {
    "papermill": {
     "duration": 0.004117,
     "end_time": "2023-01-05T21:47:04.796053",
     "exception": false,
     "start_time": "2023-01-05T21:47:04.791936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1655074e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-05T21:47:04.806773Z",
     "iopub.status.busy": "2023-01-05T21:47:04.806314Z",
     "iopub.status.idle": "2023-01-05T21:47:04.819139Z",
     "shell.execute_reply": "2023-01-05T21:47:04.818175Z"
    },
    "papermill": {
     "duration": 0.021452,
     "end_time": "2023-01-05T21:47:04.821916",
     "exception": false,
     "start_time": "2023-01-05T21:47:04.800464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataGen(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, x_arr, y, batch_size):\n",
    "        self.x = x_arr\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.batch_size\n",
    "        batch_imgs = self.x[i:i + self.batch_size,:,:]\n",
    "        batch_labs = self.y[i:i + self.batch_size,:]\n",
    "        im = np.zeros((self.batch_size,) + (28, 28) , dtype=\"float32\")\n",
    "        la = np.zeros((self.batch_size,10))\n",
    "        j = 0\n",
    "        for j in np.arange(batch_imgs.shape[0]):\n",
    "            img_array = np.squeeze(batch_imgs[j,:,:])\n",
    "            # Perform random data augmentation\n",
    "            rand_nums = np.random.rand(2,2)\n",
    "            if rand_nums[0,0]>0.5:\n",
    "                # flip\n",
    "                if rand_nums[0,1]>0.5:\n",
    "                    img_array = np.fliplr(img_array)\n",
    "                else:\n",
    "                    img_array = np.flipud(img_array)\n",
    "            if rand_nums[1,0]>0.5:\n",
    "                # rotate\n",
    "                if rand_nums[1,1]>0.5:\n",
    "                    img_array = skimage.transform.rotate(img_array, 30)\n",
    "                else:\n",
    "                    img_array = skimage.transform.rotate(img_array, 330)\n",
    "            # Perform min/max normalization\n",
    "            img_array = (img_array - np.min(img_array))/(np.max(img_array)-np.min(img_array))\n",
    "            #\n",
    "            im[j] = img_array\n",
    "            im = np.array(im)\n",
    "            la[j] = batch_labs[j,:]\n",
    "            la = np.array(la)\n",
    "        return im, la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e3a99f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-05T21:47:04.832563Z",
     "iopub.status.busy": "2023-01-05T21:47:04.832185Z",
     "iopub.status.idle": "2023-01-05T21:47:04.839256Z",
     "shell.execute_reply": "2023-01-05T21:47:04.837961Z"
    },
    "papermill": {
     "duration": 0.015155,
     "end_time": "2023-01-05T21:47:04.841615",
     "exception": false,
     "start_time": "2023-01-05T21:47:04.826460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_samples = int(np.floor(train_data.shape[0]*0.3))\n",
    "\n",
    "train_imgs = train_data[:-val_samples,:,:]\n",
    "train_labs = training_array[:-val_samples,:]\n",
    "\n",
    "val_imgs = train_data[-val_samples:,:,:]\n",
    "val_labs = training_array[-val_samples:,:]\n",
    "\n",
    "train_gen = DataGen(train_imgs, train_labs, 64)\n",
    "valid_gen = DataGen(val_imgs, val_labs, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90cc22ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-05T21:47:04.852836Z",
     "iopub.status.busy": "2023-01-05T21:47:04.852426Z",
     "iopub.status.idle": "2023-01-05T21:47:04.870853Z",
     "shell.execute_reply": "2023-01-05T21:47:04.869458Z"
    },
    "papermill": {
     "duration": 0.027198,
     "end_time": "2023-01-05T21:47:04.873400",
     "exception": false,
     "start_time": "2023-01-05T21:47:04.846202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import Input, layers, Model\n",
    "\n",
    "def get_model(img_size):\n",
    "\n",
    "    inputs = Input(shape=img_size + (1,),dtype=tf.float16)\n",
    "    print(inputs.shape)\n",
    "\n",
    "    # [First half of the network: downsampling inputs]\n",
    "\n",
    "    # Entry block\n",
    "    e1 = layers.Conv2D(16, 3, strides=1, padding=\"same\", input_shape=(1, 28, 28, 1))(inputs)\n",
    "    e2 = layers.BatchNormalization()(e1)\n",
    "    e3 = layers.Activation(\"relu\")(e2)\n",
    "\n",
    "    e4 = layers.Conv2D(16,3, strides=1, padding='same')(e3)\n",
    "    e5 = layers.BatchNormalization()(e4)\n",
    "    e6 = layers.Activation(\"relu\")(e5)\n",
    "\n",
    "    pool_e = layers.MaxPool2D(pool_size=2, strides=2, padding='same')(e6)\n",
    "\n",
    "    #Down Block 1\n",
    "    db1conv1 = layers.Conv2D(32,3, strides=1, padding='same')(pool_e)\n",
    "    db1bn1 = layers.BatchNormalization()(db1conv1)\n",
    "    db1act1 = layers.Activation(\"relu\")(db1bn1)\n",
    "\n",
    "    db1conv2 = layers.Conv2D(32,3, strides=1, padding='same')(db1act1)\n",
    "    db1bn2 = layers.BatchNormalization()(db1conv2)\n",
    "    db1act2 = layers.Activation(\"relu\")(db1bn2)\n",
    "\n",
    "    pool_1 = layers.MaxPool2D(pool_size=2, strides=2, padding='same')(db1act2)\n",
    "\n",
    "    #Down Block 2\n",
    "    db2conv1 = layers.Conv2D(64,3, strides=1, padding='same')(pool_1)\n",
    "    db2bn1 = layers.BatchNormalization()(db2conv1)\n",
    "    db2act1 = layers.Activation(\"relu\")(db2bn1)\n",
    "\n",
    "    db2conv2 = layers.Conv2D(64,3, strides=1, padding='same')(db2act1)\n",
    "    db2bn2 = layers.BatchNormalization()(db2conv2)\n",
    "    db2act2 = layers.Activation(\"relu\")(db2bn2)\n",
    "\n",
    "\n",
    "\n",
    "    #Upsampling Block 1\n",
    "    up1up = layers.UpSampling2D(size=2)(db2act2)\n",
    "\n",
    "    up1conc = layers.concatenate([up1up, db1act2], axis=-1)\n",
    "\n",
    "    up1conv1 = layers.Conv2D(32,3,strides=1, padding=\"same\")(up1conc)\n",
    "    up1bn1 = layers.BatchNormalization()(up1conv1)\n",
    "    up1act1 = layers.Activation(\"relu\")(up1bn1)\n",
    "\n",
    "    up1conv2 = layers.Conv2D(32,3,strides=1, padding=\"same\")(up1act1)\n",
    "    up1bn2 = layers.BatchNormalization()(up1conv2)\n",
    "    up1act2 = layers.Activation(\"relu\")(up1bn2)\n",
    "\n",
    "    #Upsampling Block 0\n",
    "    up0up = layers.UpSampling2D(size=2)(up1act2)\n",
    "\n",
    "    up0conc = layers.concatenate([up0up, e6], axis=-1)\n",
    "\n",
    "    up0conv1 = layers.Conv2D(16,3,strides=1, padding=\"same\")(up0conc)\n",
    "    up0bn1 = layers.BatchNormalization()(up0conv1)\n",
    "    up0act1 = layers.Activation(\"relu\")(up0bn1)\n",
    "\n",
    "    up0conv2 = layers.Conv2D(16,3,strides=1, padding=\"same\")(up0act1)\n",
    "    up0bn2 = layers.BatchNormalization()(up0conv2)\n",
    "    up0act2 = layers.Activation(\"relu\")(up0bn2)\n",
    "\n",
    "    # Exit Layer\n",
    "    econv = layers.Conv2D(1, 1, data_format=\"channels_last\")(up0act2)\n",
    "    \n",
    "    flat = keras.layers.Flatten()(econv)\n",
    "    outputs = keras.layers.Dense(10, activation='softmax')(flat)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "378f821b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-05T21:47:04.884165Z",
     "iopub.status.busy": "2023-01-05T21:47:04.883755Z",
     "iopub.status.idle": "2023-01-05T21:47:05.429051Z",
     "shell.execute_reply": "2023-01-05T21:47:05.427845Z"
    },
    "papermill": {
     "duration": 0.554767,
     "end_time": "2023-01-05T21:47:05.432686",
     "exception": false,
     "start_time": "2023-01-05T21:47:04.877919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 21:47:05.120315: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                     Output Shape                     Param #           Connected to                                      \n",
      "======================================================================================================================================================\n",
      "input_1 (InputLayer)                             [(None, 28, 28, 1)]              0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                                  (None, 28, 28, 16)               160               input_1[0][0]                                     \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization (BatchNormalization)         (None, 28, 28, 16)               64                conv2d[0][0]                                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation (Activation)                          (None, 28, 28, 16)               0                 batch_normalization[0][0]                         \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                                (None, 28, 28, 16)               2320              activation[0][0]                                  \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNormalization)       (None, 28, 28, 16)               64                conv2d_1[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_1 (Activation)                        (None, 28, 28, 16)               0                 batch_normalization_1[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)                     (None, 14, 14, 16)               0                 activation_1[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                                (None, 14, 14, 32)               4640              max_pooling2d[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNormalization)       (None, 14, 14, 32)               128               conv2d_2[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_2 (Activation)                        (None, 14, 14, 32)               0                 batch_normalization_2[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                                (None, 14, 14, 32)               9248              activation_2[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNormalization)       (None, 14, 14, 32)               128               conv2d_3[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_3 (Activation)                        (None, 14, 14, 32)               0                 batch_normalization_3[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)                   (None, 7, 7, 32)                 0                 activation_3[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                                (None, 7, 7, 64)                 18496             max_pooling2d_1[0][0]                             \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNormalization)       (None, 7, 7, 64)                 256               conv2d_4[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_4 (Activation)                        (None, 7, 7, 64)                 0                 batch_normalization_4[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                                (None, 7, 7, 64)                 36928             activation_4[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNormalization)       (None, 7, 7, 64)                 256               conv2d_5[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_5 (Activation)                        (None, 7, 7, 64)                 0                 batch_normalization_5[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)                     (None, 14, 14, 64)               0                 activation_5[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate (Concatenate)                        (None, 14, 14, 96)               0                 up_sampling2d[0][0]                               \n",
      "                                                                                                    activation_3[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                                (None, 14, 14, 32)               27680             concatenate[0][0]                                 \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNormalization)       (None, 14, 14, 32)               128               conv2d_6[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_6 (Activation)                        (None, 14, 14, 32)               0                 batch_normalization_6[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                                (None, 14, 14, 32)               9248              activation_6[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNormalization)       (None, 14, 14, 32)               128               conv2d_7[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_7 (Activation)                        (None, 14, 14, 32)               0                 batch_normalization_7[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)                   (None, 28, 28, 32)               0                 activation_7[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)                      (None, 28, 28, 48)               0                 up_sampling2d_1[0][0]                             \n",
      "                                                                                                    activation_1[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                                (None, 28, 28, 16)               6928              concatenate_1[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNormalization)       (None, 28, 28, 16)               64                conv2d_8[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_8 (Activation)                        (None, 28, 28, 16)               0                 batch_normalization_8[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                                (None, 28, 28, 16)               2320              activation_8[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNormalization)       (None, 28, 28, 16)               64                conv2d_9[0][0]                                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "activation_9 (Activation)                        (None, 28, 28, 16)               0                 batch_normalization_9[0][0]                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)                               (None, 28, 28, 1)                17                activation_9[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "flatten (Flatten)                                (None, 784)                      0                 conv2d_10[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dense (Dense)                                    (None, 10)                       7850              flatten[0][0]                                     \n",
      "======================================================================================================================================================\n",
      "Total params: 127,115\n",
      "Trainable params: 126,475\n",
      "Non-trainable params: 640\n",
      "______________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model((28,28))\n",
    "a=model.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3ab8720",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-05T21:47:05.444223Z",
     "iopub.status.busy": "2023-01-05T21:47:05.443831Z",
     "iopub.status.idle": "2023-01-05T23:44:01.767931Z",
     "shell.execute_reply": "2023-01-05T23:44:01.766613Z"
    },
    "papermill": {
     "duration": 7016.333027,
     "end_time": "2023-01-05T23:44:01.770587",
     "exception": false,
     "start_time": "2023-01-05T21:47:05.437560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 21:47:06.472540: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "460/460 [==============================] - 76s 161ms/step - loss: 0.7578 - categorical_accuracy: 0.7454 - val_loss: 0.6593 - val_categorical_accuracy: 0.7944\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65931, saving model to test1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 21:48:24.671452: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "460/460 [==============================] - 73s 158ms/step - loss: 0.3282 - categorical_accuracy: 0.8937 - val_loss: 0.3895 - val_categorical_accuracy: 0.8877\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65931 to 0.38954, saving model to test1\n",
      "Epoch 3/300\n",
      "460/460 [==============================] - 72s 157ms/step - loss: 0.2477 - categorical_accuracy: 0.9205 - val_loss: 0.3826 - val_categorical_accuracy: 0.8767\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.38954 to 0.38259, saving model to test1\n",
      "Epoch 4/300\n",
      "460/460 [==============================] - 72s 157ms/step - loss: 0.2121 - categorical_accuracy: 0.9314 - val_loss: 0.2335 - val_categorical_accuracy: 0.9256\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.38259 to 0.23347, saving model to test1\n",
      "Epoch 5/300\n",
      "460/460 [==============================] - 72s 156ms/step - loss: 0.1915 - categorical_accuracy: 0.9383 - val_loss: 0.2178 - val_categorical_accuracy: 0.9340\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.23347 to 0.21782, saving model to test1\n",
      "Epoch 6/300\n",
      "460/460 [==============================] - 76s 164ms/step - loss: 0.1700 - categorical_accuracy: 0.9430 - val_loss: 0.1956 - val_categorical_accuracy: 0.9361\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.21782 to 0.19556, saving model to test1\n",
      "Epoch 7/300\n",
      "460/460 [==============================] - 72s 157ms/step - loss: 0.1578 - categorical_accuracy: 0.9484 - val_loss: 0.1875 - val_categorical_accuracy: 0.9435\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.19556 to 0.18749, saving model to test1\n",
      "Epoch 8/300\n",
      "460/460 [==============================] - 71s 155ms/step - loss: 0.1393 - categorical_accuracy: 0.9547 - val_loss: 0.2140 - val_categorical_accuracy: 0.9342\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.18749\n",
      "Epoch 9/300\n",
      "460/460 [==============================] - 72s 156ms/step - loss: 0.1432 - categorical_accuracy: 0.9518 - val_loss: 0.2406 - val_categorical_accuracy: 0.9268\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.18749\n",
      "Epoch 10/300\n",
      "460/460 [==============================] - 72s 157ms/step - loss: 0.1323 - categorical_accuracy: 0.9577 - val_loss: 0.1738 - val_categorical_accuracy: 0.9435\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.18749 to 0.17379, saving model to test1\n",
      "Epoch 11/300\n",
      "460/460 [==============================] - 71s 154ms/step - loss: 0.1285 - categorical_accuracy: 0.9592 - val_loss: 0.1471 - val_categorical_accuracy: 0.9552\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.17379 to 0.14707, saving model to test1\n",
      "Epoch 12/300\n",
      "460/460 [==============================] - 70s 152ms/step - loss: 0.1356 - categorical_accuracy: 0.9552 - val_loss: 0.1258 - val_categorical_accuracy: 0.9607\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.14707 to 0.12579, saving model to test1\n",
      "Epoch 13/300\n",
      "460/460 [==============================] - 72s 157ms/step - loss: 0.1271 - categorical_accuracy: 0.9590 - val_loss: 0.1528 - val_categorical_accuracy: 0.9523\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.12579\n",
      "Epoch 14/300\n",
      "460/460 [==============================] - 73s 158ms/step - loss: 0.1321 - categorical_accuracy: 0.9565 - val_loss: 0.1510 - val_categorical_accuracy: 0.9490\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.12579\n",
      "Epoch 15/300\n",
      "460/460 [==============================] - 71s 155ms/step - loss: 0.1268 - categorical_accuracy: 0.9587 - val_loss: 0.1747 - val_categorical_accuracy: 0.9470\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.12579\n",
      "Epoch 16/300\n",
      "460/460 [==============================] - 70s 152ms/step - loss: 0.1218 - categorical_accuracy: 0.9601 - val_loss: 0.1789 - val_categorical_accuracy: 0.9469\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.12579\n",
      "Epoch 17/300\n",
      "460/460 [==============================] - 70s 152ms/step - loss: 0.1284 - categorical_accuracy: 0.9584 - val_loss: 0.1419 - val_categorical_accuracy: 0.9562\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.12579\n",
      "Epoch 18/300\n",
      "460/460 [==============================] - 70s 152ms/step - loss: 0.1267 - categorical_accuracy: 0.9594 - val_loss: 0.1375 - val_categorical_accuracy: 0.9579\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.12579\n",
      "Epoch 19/300\n",
      "460/460 [==============================] - 70s 152ms/step - loss: 0.1368 - categorical_accuracy: 0.9573 - val_loss: 0.1367 - val_categorical_accuracy: 0.9596\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.12579\n",
      "Epoch 20/300\n",
      "460/460 [==============================] - 70s 151ms/step - loss: 0.1137 - categorical_accuracy: 0.9646 - val_loss: 0.3722 - val_categorical_accuracy: 0.9134\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.12579\n",
      "Epoch 21/300\n",
      "460/460 [==============================] - 70s 152ms/step - loss: 0.1395 - categorical_accuracy: 0.9566 - val_loss: 0.3875 - val_categorical_accuracy: 0.8991\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.12579\n",
      "Epoch 22/300\n",
      "460/460 [==============================] - 70s 151ms/step - loss: 0.1407 - categorical_accuracy: 0.9585 - val_loss: 0.1285 - val_categorical_accuracy: 0.9659\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.12579\n",
      "Epoch 23/300\n",
      "460/460 [==============================] - 70s 152ms/step - loss: 0.0757 - categorical_accuracy: 0.9750 - val_loss: 0.0916 - val_categorical_accuracy: 0.9715\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.12579 to 0.09157, saving model to test1\n",
      "Epoch 24/300\n",
      "460/460 [==============================] - 72s 156ms/step - loss: 0.0702 - categorical_accuracy: 0.9773 - val_loss: 0.0851 - val_categorical_accuracy: 0.9739\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.09157 to 0.08513, saving model to test1\n",
      "Epoch 25/300\n",
      "460/460 [==============================] - 72s 156ms/step - loss: 0.0668 - categorical_accuracy: 0.9777 - val_loss: 0.0920 - val_categorical_accuracy: 0.9726\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.08513\n",
      "Epoch 26/300\n",
      "460/460 [==============================] - 72s 157ms/step - loss: 0.0624 - categorical_accuracy: 0.9798 - val_loss: 0.0871 - val_categorical_accuracy: 0.9736\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.08513\n",
      "Epoch 27/300\n",
      "460/460 [==============================] - 72s 157ms/step - loss: 0.0615 - categorical_accuracy: 0.9791 - val_loss: 0.0882 - val_categorical_accuracy: 0.9732\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.08513\n",
      "Epoch 28/300\n",
      "460/460 [==============================] - 75s 162ms/step - loss: 0.0609 - categorical_accuracy: 0.9801 - val_loss: 0.0845 - val_categorical_accuracy: 0.9754\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.08513 to 0.08448, saving model to test1\n",
      "Epoch 29/300\n",
      "460/460 [==============================] - 76s 166ms/step - loss: 0.0620 - categorical_accuracy: 0.9789 - val_loss: 0.0836 - val_categorical_accuracy: 0.9753\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.08448 to 0.08356, saving model to test1\n",
      "Epoch 30/300\n",
      "460/460 [==============================] - 74s 162ms/step - loss: 0.0571 - categorical_accuracy: 0.9804 - val_loss: 0.0795 - val_categorical_accuracy: 0.9752\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.08356 to 0.07946, saving model to test1\n",
      "Epoch 31/300\n",
      "460/460 [==============================] - 74s 160ms/step - loss: 0.0575 - categorical_accuracy: 0.9819 - val_loss: 0.0835 - val_categorical_accuracy: 0.9744\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.07946\n",
      "Epoch 32/300\n",
      "460/460 [==============================] - 73s 159ms/step - loss: 0.0571 - categorical_accuracy: 0.9810 - val_loss: 0.0810 - val_categorical_accuracy: 0.9751\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.07946\n",
      "Epoch 33/300\n",
      "460/460 [==============================] - 73s 158ms/step - loss: 0.0561 - categorical_accuracy: 0.9818 - val_loss: 0.0849 - val_categorical_accuracy: 0.9735\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.07946\n",
      "Epoch 34/300\n",
      "460/460 [==============================] - 72s 157ms/step - loss: 0.0562 - categorical_accuracy: 0.9829 - val_loss: 0.0866 - val_categorical_accuracy: 0.9748\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.07946\n",
      "Epoch 35/300\n",
      "460/460 [==============================] - 69s 151ms/step - loss: 0.0509 - categorical_accuracy: 0.9831 - val_loss: 0.0803 - val_categorical_accuracy: 0.9765\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.07946\n",
      "Epoch 36/300\n",
      "460/460 [==============================] - 71s 154ms/step - loss: 0.0529 - categorical_accuracy: 0.9823 - val_loss: 0.0787 - val_categorical_accuracy: 0.9756\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.07946 to 0.07874, saving model to test1\n",
      "Epoch 37/300\n",
      "460/460 [==============================] - 69s 151ms/step - loss: 0.0521 - categorical_accuracy: 0.9826 - val_loss: 0.0835 - val_categorical_accuracy: 0.9747\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.07874\n",
      "Epoch 38/300\n",
      "460/460 [==============================] - 71s 154ms/step - loss: 0.0534 - categorical_accuracy: 0.9823 - val_loss: 0.0838 - val_categorical_accuracy: 0.9757\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.07874\n",
      "Epoch 39/300\n",
      "460/460 [==============================] - 71s 155ms/step - loss: 0.0528 - categorical_accuracy: 0.9840 - val_loss: 0.0853 - val_categorical_accuracy: 0.9757\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.07874\n",
      "Epoch 40/300\n",
      "460/460 [==============================] - 72s 156ms/step - loss: 0.0488 - categorical_accuracy: 0.9837 - val_loss: 0.0821 - val_categorical_accuracy: 0.9754\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.07874\n",
      "Epoch 41/300\n",
      "460/460 [==============================] - 71s 154ms/step - loss: 0.0523 - categorical_accuracy: 0.9828 - val_loss: 0.0783 - val_categorical_accuracy: 0.9787\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.07874 to 0.07827, saving model to test1\n",
      "Epoch 42/300\n",
      "460/460 [==============================] - 70s 152ms/step - loss: 0.0453 - categorical_accuracy: 0.9845 - val_loss: 0.0790 - val_categorical_accuracy: 0.9785\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.07827\n",
      "Epoch 43/300\n",
      "460/460 [==============================] - 72s 157ms/step - loss: 0.0464 - categorical_accuracy: 0.9856 - val_loss: 0.0870 - val_categorical_accuracy: 0.9737\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.07827\n",
      "Epoch 44/300\n",
      "460/460 [==============================] - 72s 155ms/step - loss: 0.0458 - categorical_accuracy: 0.9839 - val_loss: 0.0816 - val_categorical_accuracy: 0.9767\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.07827\n",
      "Epoch 45/300\n",
      "460/460 [==============================] - 75s 162ms/step - loss: 0.0482 - categorical_accuracy: 0.9834 - val_loss: 0.0829 - val_categorical_accuracy: 0.9758\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.07827\n",
      "Epoch 46/300\n",
      "460/460 [==============================] - 74s 161ms/step - loss: 0.0453 - categorical_accuracy: 0.9846 - val_loss: 0.0781 - val_categorical_accuracy: 0.9780\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.07827 to 0.07811, saving model to test1\n",
      "Epoch 47/300\n",
      "460/460 [==============================] - 75s 162ms/step - loss: 0.0462 - categorical_accuracy: 0.9841 - val_loss: 0.0824 - val_categorical_accuracy: 0.9767\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.07811\n",
      "Epoch 48/300\n",
      "460/460 [==============================] - 74s 161ms/step - loss: 0.0435 - categorical_accuracy: 0.9856 - val_loss: 0.0791 - val_categorical_accuracy: 0.9776\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.07811\n",
      "Epoch 49/300\n",
      "460/460 [==============================] - 74s 161ms/step - loss: 0.0458 - categorical_accuracy: 0.9852 - val_loss: 0.0822 - val_categorical_accuracy: 0.9769\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.07811\n",
      "Epoch 50/300\n",
      "460/460 [==============================] - 74s 160ms/step - loss: 0.0446 - categorical_accuracy: 0.9847 - val_loss: 0.0875 - val_categorical_accuracy: 0.9761\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.07811\n",
      "Epoch 51/300\n",
      "460/460 [==============================] - 75s 161ms/step - loss: 0.0425 - categorical_accuracy: 0.9857 - val_loss: 0.0840 - val_categorical_accuracy: 0.9766\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.07811\n",
      "Epoch 52/300\n",
      "460/460 [==============================] - 74s 160ms/step - loss: 0.0416 - categorical_accuracy: 0.9856 - val_loss: 0.0785 - val_categorical_accuracy: 0.9778\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.07811\n",
      "Epoch 53/300\n",
      "460/460 [==============================] - 73s 159ms/step - loss: 0.0417 - categorical_accuracy: 0.9859 - val_loss: 0.0806 - val_categorical_accuracy: 0.9757\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.07811\n",
      "Epoch 54/300\n",
      "460/460 [==============================] - 74s 161ms/step - loss: 0.0402 - categorical_accuracy: 0.9860 - val_loss: 0.0806 - val_categorical_accuracy: 0.9764\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.07811\n",
      "Epoch 55/300\n",
      "460/460 [==============================] - 73s 158ms/step - loss: 0.0427 - categorical_accuracy: 0.9855 - val_loss: 0.0887 - val_categorical_accuracy: 0.9749\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.07811\n",
      "Epoch 56/300\n",
      "460/460 [==============================] - 74s 162ms/step - loss: 0.0404 - categorical_accuracy: 0.9860 - val_loss: 0.0826 - val_categorical_accuracy: 0.9763\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.07811\n",
      "Epoch 57/300\n",
      "460/460 [==============================] - 74s 161ms/step - loss: 0.0390 - categorical_accuracy: 0.9865 - val_loss: 0.0807 - val_categorical_accuracy: 0.9783\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.07811\n",
      "Epoch 58/300\n",
      "460/460 [==============================] - 73s 160ms/step - loss: 0.0389 - categorical_accuracy: 0.9869 - val_loss: 0.0843 - val_categorical_accuracy: 0.9776\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.07811\n",
      "Epoch 59/300\n",
      "460/460 [==============================] - 75s 164ms/step - loss: 0.0378 - categorical_accuracy: 0.9873 - val_loss: 0.0843 - val_categorical_accuracy: 0.9764\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.07811\n",
      "Epoch 60/300\n",
      "460/460 [==============================] - 74s 161ms/step - loss: 0.0353 - categorical_accuracy: 0.9881 - val_loss: 0.0796 - val_categorical_accuracy: 0.9778\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.07811\n",
      "Epoch 61/300\n",
      "460/460 [==============================] - 73s 159ms/step - loss: 0.0343 - categorical_accuracy: 0.9879 - val_loss: 0.0833 - val_categorical_accuracy: 0.9772\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.07811\n",
      "Epoch 62/300\n",
      "460/460 [==============================] - 74s 161ms/step - loss: 0.0364 - categorical_accuracy: 0.9875 - val_loss: 0.0855 - val_categorical_accuracy: 0.9779\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.07811\n",
      "Epoch 63/300\n",
      "460/460 [==============================] - 74s 162ms/step - loss: 0.0363 - categorical_accuracy: 0.9878 - val_loss: 0.0839 - val_categorical_accuracy: 0.9779\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.07811\n",
      "Epoch 64/300\n",
      "460/460 [==============================] - 73s 159ms/step - loss: 0.0340 - categorical_accuracy: 0.9879 - val_loss: 0.0782 - val_categorical_accuracy: 0.9780\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.07811\n",
      "Epoch 65/300\n",
      "460/460 [==============================] - 74s 161ms/step - loss: 0.0361 - categorical_accuracy: 0.9877 - val_loss: 0.0742 - val_categorical_accuracy: 0.9780\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.07811 to 0.07417, saving model to test1\n",
      "Epoch 66/300\n",
      "460/460 [==============================] - 74s 161ms/step - loss: 0.0380 - categorical_accuracy: 0.9863 - val_loss: 0.0719 - val_categorical_accuracy: 0.9790\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.07417 to 0.07191, saving model to test1\n",
      "Epoch 67/300\n",
      "460/460 [==============================] - 74s 162ms/step - loss: 0.0360 - categorical_accuracy: 0.9879 - val_loss: 0.0808 - val_categorical_accuracy: 0.9776\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.07191\n",
      "Epoch 68/300\n",
      "460/460 [==============================] - 74s 162ms/step - loss: 0.0357 - categorical_accuracy: 0.9878 - val_loss: 0.0839 - val_categorical_accuracy: 0.9775\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.07191\n",
      "Epoch 69/300\n",
      "460/460 [==============================] - 76s 166ms/step - loss: 0.0353 - categorical_accuracy: 0.9875 - val_loss: 0.0780 - val_categorical_accuracy: 0.9785\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.07191\n",
      "Epoch 70/300\n",
      "460/460 [==============================] - 74s 162ms/step - loss: 0.0379 - categorical_accuracy: 0.9868 - val_loss: 0.0759 - val_categorical_accuracy: 0.9800\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.07191\n",
      "Epoch 71/300\n",
      "460/460 [==============================] - 75s 162ms/step - loss: 0.0356 - categorical_accuracy: 0.9882 - val_loss: 0.0832 - val_categorical_accuracy: 0.9782\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.07191\n",
      "Epoch 72/300\n",
      "460/460 [==============================] - 75s 163ms/step - loss: 0.0357 - categorical_accuracy: 0.9878 - val_loss: 0.0765 - val_categorical_accuracy: 0.9774\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.07191\n",
      "Epoch 73/300\n",
      "460/460 [==============================] - 77s 167ms/step - loss: 0.0346 - categorical_accuracy: 0.9874 - val_loss: 0.0798 - val_categorical_accuracy: 0.9768\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.07191\n",
      "Epoch 74/300\n",
      "460/460 [==============================] - 77s 168ms/step - loss: 0.0340 - categorical_accuracy: 0.9884 - val_loss: 0.0762 - val_categorical_accuracy: 0.9781\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.07191\n",
      "Epoch 75/300\n",
      "460/460 [==============================] - 75s 163ms/step - loss: 0.0361 - categorical_accuracy: 0.9875 - val_loss: 0.0821 - val_categorical_accuracy: 0.9773\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.07191\n",
      "Epoch 76/300\n",
      "460/460 [==============================] - 76s 166ms/step - loss: 0.0374 - categorical_accuracy: 0.9868 - val_loss: 0.0784 - val_categorical_accuracy: 0.9778\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.07191\n",
      "Epoch 77/300\n",
      "460/460 [==============================] - 76s 165ms/step - loss: 0.0374 - categorical_accuracy: 0.9875 - val_loss: 0.0782 - val_categorical_accuracy: 0.9780\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.07191\n",
      "Epoch 78/300\n",
      "460/460 [==============================] - 76s 166ms/step - loss: 0.0377 - categorical_accuracy: 0.9875 - val_loss: 0.0809 - val_categorical_accuracy: 0.9782\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.07191\n",
      "Epoch 79/300\n",
      "460/460 [==============================] - 74s 162ms/step - loss: 0.0346 - categorical_accuracy: 0.9886 - val_loss: 0.0787 - val_categorical_accuracy: 0.9800\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.07191\n",
      "Epoch 80/300\n",
      "460/460 [==============================] - 75s 163ms/step - loss: 0.0351 - categorical_accuracy: 0.9879 - val_loss: 0.0771 - val_categorical_accuracy: 0.9793\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.07191\n",
      "Epoch 81/300\n",
      "460/460 [==============================] - 75s 164ms/step - loss: 0.0382 - categorical_accuracy: 0.9874 - val_loss: 0.0821 - val_categorical_accuracy: 0.9776\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.07191\n",
      "Epoch 82/300\n",
      "460/460 [==============================] - 74s 161ms/step - loss: 0.0335 - categorical_accuracy: 0.9887 - val_loss: 0.0830 - val_categorical_accuracy: 0.9783\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.07191\n",
      "Epoch 83/300\n",
      "460/460 [==============================] - 75s 163ms/step - loss: 0.0352 - categorical_accuracy: 0.9878 - val_loss: 0.0756 - val_categorical_accuracy: 0.9787\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.07191\n",
      "Epoch 84/300\n",
      "460/460 [==============================] - 74s 161ms/step - loss: 0.0347 - categorical_accuracy: 0.9886 - val_loss: 0.0785 - val_categorical_accuracy: 0.9779\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.07191\n",
      "Epoch 85/300\n",
      "460/460 [==============================] - 76s 166ms/step - loss: 0.0367 - categorical_accuracy: 0.9875 - val_loss: 0.0843 - val_categorical_accuracy: 0.9776\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.07191\n",
      "Epoch 86/300\n",
      "460/460 [==============================] - 75s 162ms/step - loss: 0.0351 - categorical_accuracy: 0.9883 - val_loss: 0.0836 - val_categorical_accuracy: 0.9783\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.07191\n",
      "Epoch 00086: early stopping\n",
      "Training time:  7016.300229549408\n"
     ]
    }
   ],
   "source": [
    "import time, math\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=0.01), loss=['categorical_crossentropy'],metrics = ['categorical_accuracy'])\n",
    "epochs = 300\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=20, verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00000001, verbose=1),\n",
    "    keras.callbacks.ModelCheckpoint(\"test1\", verbose=1, save_best_only=True)\n",
    "]\n",
    "start = time.time()\n",
    "history = model.fit(train_gen, epochs=epochs, validation_data=valid_gen, callbacks=callbacks,shuffle=True)\n",
    "end = time.time()\n",
    "print('Training time: ', end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7070.468362,
   "end_time": "2023-01-05T23:44:06.986556",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-05T21:46:16.518194",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
