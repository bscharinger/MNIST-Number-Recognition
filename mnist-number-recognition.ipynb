{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2023-01-05T15:47:38.406405Z","iopub.execute_input":"2023-01-05T15:47:38.406810Z","iopub.status.idle":"2023-01-05T15:47:44.472035Z","shell.execute_reply.started":"2023-01-05T15:47:38.406778Z","shell.execute_reply":"2023-01-05T15:47:44.470844Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ntrain_data = np.genfromtxt('/kaggle/input/digit-recognizer/train.csv', delimiter=',')","metadata":{"execution":{"iopub.status.busy":"2023-01-05T15:19:16.580542Z","iopub.execute_input":"2023-01-05T15:19:16.580947Z","iopub.status.idle":"2023-01-05T15:19:49.115053Z","shell.execute_reply.started":"2023-01-05T15:19:16.580916Z","shell.execute_reply":"2023-01-05T15:19:49.113616Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_labels = train_data[1:,0]\ntrain_data = train_data[1:,1:]","metadata":{"execution":{"iopub.status.busy":"2023-01-05T15:25:02.554846Z","iopub.execute_input":"2023-01-05T15:25:02.555251Z","iopub.status.idle":"2023-01-05T15:25:02.561040Z","shell.execute_reply.started":"2023-01-05T15:25:02.555219Z","shell.execute_reply":"2023-01-05T15:25:02.559665Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(train_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T15:26:22.885228Z","iopub.execute_input":"2023-01-05T15:26:22.885686Z","iopub.status.idle":"2023-01-05T15:26:22.892028Z","shell.execute_reply.started":"2023-01-05T15:26:22.885605Z","shell.execute_reply":"2023-01-05T15:26:22.890549Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"(41999,)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = train_data.reshape(-1,28,28,1)\nprint(train_data.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T15:26:08.240379Z","iopub.execute_input":"2023-01-05T15:26:08.240817Z","iopub.status.idle":"2023-01-05T15:26:08.247400Z","shell.execute_reply.started":"2023-01-05T15:26:08.240775Z","shell.execute_reply":"2023-01-05T15:26:08.246016Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"(41999, 28, 28, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"class DataGen(keras.utils.Sequence):\n\n    def __init__(self, x_arr, batch_size):\n        self.x = x_arr\n        self.batch_size = batch_size\n    def __len__(self):\n        return math.ceil(len(self.x) / self.batch_size)\n\n    def __getitem__(self, idx):\n        i = idx * self.batch_size\n        batch_img_paths = self.x[i:i + self.batch_size]\n        im = np.zeros((self.batch_size,) + (28, 28) , dtype=\"float32\")\n        la = np.zeros((self.batch_size,) + (1,))\n        j = 0\n        for j, path in enumerate(batch_img_paths):\n            img = self.x_arr\n            img_array = keras.preprocessing.image.img_to_array(img)\n            img_array = np.squeeze(img_array)\n            # Perform random data augmentation\n            rand_nums = np.random.rand(2,2)\n            if rand_nums[0,0]>0.5:\n                # flip\n                if rand_nums[0,1]>0.5:\n                    img_array = np.fliplr(img_array)\n                else:\n                    img_array = np.flipud(img_array)\n            if rand_nums[1,0]>0.5:\n                # rotate\n                if rand_nums[1,1]>0.5:\n                    img_array = skimage.transform.rotate(img_array, 30)\n                else:\n                    img_array = skimage.transform.rotate(img_array, 330)\n\n            # Perform min/max normalization\n            img_array = (img_array - np.min(img_array))/(np.max(img_array)-np.min(img_array))\n            #\n            im[j] = img_array\n            im = np.array(im)\n            id = int(path.split('\\\\')[-1].split('.')[0])\n            label = np.squeeze(train_df.loc[train_df['image_id'] == id, ['cancer']].values)\n            la[j] = label\n            la = np.array(la)\n        return im, la","metadata":{},"execution_count":null,"outputs":[]}]}